{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libaries\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, DatasetDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'why is crop rotation important in farming?', 'answers': 'This helps to prevent soil erosion and depletion, and can also help to control pests and diseases'}\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"KisanVaani/agriculture-qa-english-only\")\n",
    "print(ds[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Main preprocessing function that coordinates all preprocessing steps\n",
    "    \"\"\"\n",
    "    # Convert dataset to pandas DataFrame for easier manipulation\n",
    "    df = pd.DataFrame({\n",
    "        'question': [item['question'] for item in dataset],\n",
    "        'answer': [item['answer'] for item in dataset]\n",
    "    })\n",
    "    \n",
    "    # Clean text\n",
    "    df['question_cleaned'] = df['question'].apply(clean_text)\n",
    "    df['answer_cleaned'] = df['answer'].apply(clean_text)\n",
    "    \n",
    "    # Initialize tokenizer (using BERT base model)\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    # Tokenize the cleaned text\n",
    "    encoded_data = tokenize_data(df, tokenizer)\n",
    "    \n",
    "    return encoded_data, tokenizer\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Comprehensive text cleaning function\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    \n",
    "    # Normalize unicode characters\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "def tokenize_data(df, tokenizer, max_length=128):\n",
    "    \"\"\"\n",
    "    Tokenize the cleaned text data using the BERT tokenizer\n",
    "    \"\"\"\n",
    "    # Tokenize questions\n",
    "    questions_encoded = tokenizer(\n",
    "        df['question_cleaned'].tolist(),\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=max_length,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    \n",
    "    # Tokenize answers\n",
    "    answers_encoded = tokenizer(\n",
    "        df['answer_cleaned'].tolist(),\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=max_length,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'input_ids': questions_encoded['input_ids'],\n",
    "        'attention_mask': questions_encoded['attention_mask'],\n",
    "        'labels': answers_encoded['input_ids']\n",
    "    }\n",
    "\n",
    "def create_train_val_test_split(encoded_data, train_size=0.7, val_size=0.15):\n",
    "    \"\"\"\n",
    "    Split the encoded data into train, validation, and test sets\n",
    "    \"\"\"\n",
    "    # First split into train and temp\n",
    "    train_data, temp_data = {}, {}\n",
    "    \n",
    "    for key in encoded_data:\n",
    "        train_data[key], temp_data[key] = train_test_split(\n",
    "            encoded_data[key],\n",
    "            train_size=train_size,\n",
    "            random_state=42\n",
    "        )\n",
    "    \n",
    "    # Split temp into validation and test\n",
    "    val_data, test_data = {}, {}\n",
    "    val_ratio = val_size / (1 - train_size)\n",
    "    \n",
    "    for key in temp_data:\n",
    "        val_data[key], test_data[key] = train_test_split(\n",
    "            temp_data[key],\n",
    "            train_size=val_ratio,\n",
    "            random_state=42\n",
    "        )\n",
    "    \n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "\n",
    "class HyperparameterTuner:\n",
    "    def __init__(self, train_data, val_data, tokenizer):\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.results = []\n",
    "        \n",
    "    def create_model(self, learning_rate, dropout_rate):\n",
    "        \"\"\"\n",
    "        Create model with specified hyperparameters\n",
    "        \"\"\"\n",
    "        model = TFAutoModelForSeq2SeqLM.from_pretrained('t5-small')\n",
    "        \n",
    "        # Adjust model configuration\n",
    "        model.config.dropout_rate = dropout_rate\n",
    "        \n",
    "        # Compile model\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy')\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train_and_evaluate(self, model, batch_size, epochs):\n",
    "        \"\"\"\n",
    "        Train model and evaluate performance\n",
    "        \"\"\"\n",
    "        # Convert data to tf.data.Dataset\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(self.train_data).batch(batch_size)\n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices(self.val_data).batch(batch_size)\n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            train_dataset,\n",
    "            validation_data=val_dataset,\n",
    "            epochs=epochs,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Generate predictions on validation set\n",
    "        val_predictions = model.predict(val_dataset)\n",
    "        \n",
    "        # Convert predictions to text\n",
    "        predicted_text = self.tokenizer.batch_decode(\n",
    "            tf.argmax(val_predictions, axis=-1),\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        actual_text = self.tokenizer.batch_decode(\n",
    "            self.val_data['labels'],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'val_loss': min(history.history['val_loss']),\n",
    "            'accuracy': accuracy_score(actual_text, predicted_text),\n",
    "            'f1_score': f1_score(actual_text, predicted_text, average='weighted')\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def run_grid_search(self, param_grid):\n",
    "        \"\"\"\n",
    "        Perform grid search over hyperparameter combinations\n",
    "        \"\"\"\n",
    "        # Generate all combinations of hyperparameters\n",
    "        param_combinations = [dict(zip(param_grid.keys(), v)) \n",
    "                            for v in itertools.product(*param_grid.values())]\n",
    "        \n",
    "        for params in param_combinations:\n",
    "            # Create and train model\n",
    "            model = self.create_model(\n",
    "                learning_rate=params['learning_rate'],\n",
    "                dropout_rate=params['dropout_rate']\n",
    "            )\n",
    "            \n",
    "            metrics = self.train_and_evaluate(\n",
    "                model,\n",
    "                batch_size=params['batch_size'],\n",
    "                epochs=params['epochs']\n",
    "            )\n",
    "            \n",
    "            # Store results\n",
    "            self.results.append({\n",
    "                'timestamp': datetime.now(),\n",
    "                **params,\n",
    "                **metrics\n",
    "            })\n",
    "    \n",
    "    def get_results_df(self):\n",
    "        \"\"\"\n",
    "        Return results as pandas DataFrame\n",
    "        \"\"\"\n",
    "        return pd.DataFrame(self.results)\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [1e-4, 3e-4, 1e-3],\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'dropout_rate': [0.1, 0.2, 0.3],\n",
    "    'epochs': [3, 5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract questions and answers\n",
    "\n",
    "questions = [item['question'] for item in ds['train']]\n",
    "answers = [item['answers'] for item in ds['train']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tune a transformer model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ChatbotEvaluator:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "        self.gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        \n",
    "    def calculate_f1_metrics(self, reference, candidate):\n",
    "        \"\"\"\n",
    "        Calculate F1, precision, and recall scores for a single response\n",
    "        \"\"\"\n",
    "        # Tokenize both reference and candidate\n",
    "        reference_tokens = set(word_tokenize(reference.lower()))\n",
    "        candidate_tokens = set(word_tokenize(candidate.lower()))\n",
    "        \n",
    "        # Calculate true positives, false positives, and false negatives\n",
    "        true_positives = len(reference_tokens.intersection(candidate_tokens))\n",
    "        false_positives = len(candidate_tokens - reference_tokens)\n",
    "        false_negatives = len(reference_tokens - candidate_tokens)\n",
    "        \n",
    "        # Calculate precision\n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "        \n",
    "        # Calculate recall\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "        \n",
    "        # Calculate F1 score\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'f1': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall\n",
    "        }\n",
    "    \n",
    "    def calculate_perplexity(self, text):\n",
    "        \"\"\"\n",
    "        Calculate perplexity using GPT-2 as a language model\n",
    "        \"\"\"\n",
    "        encodings = self.gpt2_tokenizer(text, return_tensors='pt')\n",
    "        max_length = 1024\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.gpt2_model(**encodings)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "        return torch.exp(loss).item()\n",
    "    \n",
    "    def semantic_similarity(self, text1, text2):\n",
    "        \"\"\"\n",
    "        Calculate semantic similarity between two texts using cosine similarity\n",
    "        \"\"\"\n",
    "        encoding1 = self.tokenizer(text1, return_tensors='tf', padding=True, truncation=True)\n",
    "        encoding2 = self.tokenizer(text2, return_tensors='tf', padding=True, truncation=True)\n",
    "        \n",
    "        embeddings1 = self.model.encode(encoding1)\n",
    "        embeddings2 = self.model.encode(encoding2)\n",
    "        \n",
    "        similarity = np.dot(embeddings1, embeddings2) / (np.linalg.norm(embeddings1) * np.linalg.norm(embeddings2))\n",
    "        return similarity\n",
    "    \n",
    "    def evaluate_responses(self, true_responses, predicted_responses):\n",
    "        \"\"\"\n",
    "        Comprehensive evaluation of model responses\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            'f1_scores': [],\n",
    "            'precision_scores': [],\n",
    "            'recall_scores': [],\n",
    "            'perplexities': [],\n",
    "            'semantic_similarities': [],\n",
    "            'response_lengths': []\n",
    "        }\n",
    "        \n",
    "        for true_resp, pred_resp in tqdm(zip(true_responses, predicted_responses)):\n",
    "            # Calculate F1, precision, and recall\n",
    "            metrics = self.calculate_f1_metrics(true_resp, pred_resp)\n",
    "            results['f1_scores'].append(metrics['f1'])\n",
    "            results['precision_scores'].append(metrics['precision'])\n",
    "            results['recall_scores'].append(metrics['recall'])\n",
    "            \n",
    "            # Calculate perplexity\n",
    "            perplexity = self.calculate_perplexity(pred_resp)\n",
    "            results['perplexities'].append(perplexity)\n",
    "            \n",
    "            # Calculate semantic similarity\n",
    "            similarity = self.semantic_similarity(true_resp, pred_resp)\n",
    "            results['semantic_similarities'].append(similarity)\n",
    "            \n",
    "            # Response length analysis\n",
    "            results['response_lengths'].append(len(word_tokenize(pred_resp)))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def generate_evaluation_report(self, results):\n",
    "        \"\"\"\n",
    "        Generate comprehensive evaluation report with visualizations\n",
    "        \"\"\"\n",
    "        report = {\n",
    "            'metrics': {\n",
    "                'avg_f1': np.mean(results['f1_scores']),\n",
    "                'avg_precision': np.mean(results['precision_scores']),\n",
    "                'avg_recall': np.mean(results['recall_scores']),\n",
    "                'avg_perplexity': np.mean(results['perplexities']),\n",
    "                'avg_similarity': np.mean(results['semantic_similarities']),\n",
    "                'avg_length': np.mean(results['response_lengths'])\n",
    "            },\n",
    "            'distributions': results\n",
    "        }\n",
    "        \n",
    "        # Create visualizations\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # F1 score distribution\n",
    "        plt.subplot(2, 3, 1)\n",
    "        sns.histplot(results['f1_scores'])\n",
    "        plt.title('F1 Score Distribution')\n",
    "        \n",
    "        # Precision distribution\n",
    "        plt.subplot(2, 3, 2)\n",
    "        sns.histplot(results['precision_scores'])\n",
    "        plt.title('Precision Distribution')\n",
    "        \n",
    "        # Recall distribution\n",
    "        plt.subplot(2, 3, 3)\n",
    "        sns.histplot(results['recall_scores'])\n",
    "        plt.title('Recall Distribution')\n",
    "        \n",
    "        # Perplexity distribution\n",
    "        plt.subplot(2, 3, 4)\n",
    "        sns.histplot(results['perplexities'])\n",
    "        plt.title('Perplexity Distribution')\n",
    "        \n",
    "        # Semantic similarity distribution\n",
    "        plt.subplot(2, 3, 5)\n",
    "        sns.histplot(results['semantic_similarities'])\n",
    "        plt.title('Semantic Similarity Distribution')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        return report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qualitative_analysis(predicted_responses, true_responses, n_samples=5):\n",
    "    \"\"\"\n",
    "    Perform qualitative analysis on random samples\n",
    "    \"\"\"\n",
    "    indices = np.random.choice(len(predicted_responses), n_samples, replace=False)\n",
    "    analysis = []\n",
    "    \n",
    "    evaluator = ChatbotEvaluator(None, None)  # Temporary instance for F1 calculation\n",
    "    \n",
    "    for idx in indices:\n",
    "        metrics = evaluator.calculate_f1_metrics(true_responses[idx], predicted_responses[idx])\n",
    "        analysis.append({\n",
    "            'true_response': true_responses[idx],\n",
    "            'predicted_response': predicted_responses[idx],\n",
    "            'f1_score': metrics['f1'],\n",
    "            'precision': metrics['precision'],\n",
    "            'recall': metrics['recall'],\n",
    "            'response_length': len(word_tokenize(predicted_responses[idx])),\n",
    "            'notes': ''  # For manual analysis notes\n",
    "        })\n",
    "    \n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
